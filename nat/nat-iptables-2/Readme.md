<properties
pageTitle= 'Iptables to control traffic inbound and outbound Azure VMs'
description= "Linux nva with iptables"
documentationcenter: na
services=""
documentationCenter="na"
authors="fabferri"
manager=""
editor=""/>

<tags
   ms.service="configuration-Example-Azure"
   ms.devlang="na"
   ms.topic="article"
   ms.tgt_pltfrm="na"
   ms.workload="na"
   ms.date="07/02/2021"
   ms.author="fabferri" />

# Iptables to control traffic inbound and outbound Azure VMs
The network diagram is shown below:

[![1]][1]

- The virtual network **vnet1** is in peering with **vnet2** and **vnet3**
- The VMs **nva1** and **nva2** work as NVA, with IP forwarding enabled to route the IP packets and iptables to control the the traffic inbound and outbound internet  
- the iptables in **nva1** and **nva2** are configured with destination NAT and source NAT to accept the inbound traffic from internet on port 8081 and 8081. The traffic inbound to the port 8081 is changed to be forwarded to the **vmApp2**. The inbound traffic to the port 8082 is changed to be forwarded to the **vmApp3**.
- in the **vnet1** is configured an internal load balancer in HA ports to balance the traffic between **nva1** and **nva2**
- A UDR is applied to the **appSubnet** of **vnet2** and **vnet3** to force the traffic to pass through the frontend IP of the internet load balancer.
- in each VM is installed nginx on HTTP port 80, with simple custom homepage with VM name. The nginix in **nva1** and **nva2** are required to answer to the load balancer health probes set on port 80.
- the traffic outbound from **vmApp2** and **vmApp3** to internet is source NAT (POSTROUTING chain) by iptables NAT masquerade function

<br>

The diagram below shows the inbound traffic from internet to **nva1** on destination port 8081:

[![2]][2]

The diagram below shows the inbound traffic from internet to **nva1** on destination port 8082:
[![3]][3]


The data paths between **vnet2** and **vnet3** is shown below:

[![4]][4]

The Azure load balancer supports the following distribution modes for routing connections to instances in the backed pool:

| Distribution mode | Hash based | Session persistence: Client IP | Session persistence: Client IP and protocol |
| --- | --- | --- | --- |
| Description | Traffic from the same client IP routed to any healthy instance in the backend pool | Traffic from the same client IP is routed to the same backend instance | Traffic from the same client IP and protocol is routed to the same backend instance |
| Tuples | 5 tuple | 2 tuple | 3 tuple |
| Azure portal configuration | Session persistence: **None** | Session persistence: **Client IP** | Session persistence: **Client IP and protocol** |
| REST API |  ```"loadDistribution":"Default"```| ```"loadDistribution":"SourceIP"```    | ```"loadDistribution":"SourceIPProtocol"```    |

<br>

In our setup, the ARM template the load balancer distribution mode is set to: **"loadDistribution":"SourceIP"**

## <a name="IP forwarding"></a>1. IP forwarding in NVAs
The IP forwarding in **nva1** and **nva2** is configured by adding the line **net.ipv4.ip_forward=1** in the file **/etc/sysctl.conf**:
```bash
sed -i -e '/^\(net.ipv4.ip_forward=\).*/{s//\11/;:a;n;ba;q}' -e '$anet.ipv4.ip_forward=1' /etc/sysctl.conf; sysctl -p
```
The bah command:
```bash
sed \
    -e '/^\(option=\).*/{s//\1value/;:a;n;ba;q}' \
    -e '$aoption=value' filename
```
check the presence of "option=value" in the file. If doesn't exist, it would add it to the bottom of the file.


## <a name="iptables configurations"></a>2. iptables configuration in NVAs
The iptables diagram shows how the tables and chains are traversed:

[![5]][5]

The built-in chains for the nat table are as follows:
* PREROUTING — Alters network packets when they arrive.
* OUTPUT — Alters locally-generated network packets before they are sent out.
* POSTROUTING — Alters network packets before they are sent out.
Every network packet received by or sent out of a Linux system is subject to at least one table. However, a packet may be subjected to multiple rules within each table before emerging at the end of the chain.

Below the iptables rules in use in **nva1**:
```console
iptables -I INPUT 1 -i lo -j ACCEPT;
iptables -A INPUT -p tcp --dport ssh -j ACCEPT;
iptables -A INPUT -p tcp -s 168.63.129.16 --dport 80 -j ACCEPT;
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT;
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT;
iptables -A INPUT -j DROP;
iptables -P FORWARD ACCEPT;
iptables -P OUTPUT ACCEPT;
iptables -t nat -A PREROUTING -i eth0 -s 10.2.0.0/24 -d 10.3.0.0/24  -j ACCEPT;
iptables -t nat -A PREROUTING -i eth0 -s 10.3.0.0/24 -d 10.2.0.0/24  -j ACCEPT;
iptables -t nat -A POSTROUTING -o eth0 -s 10.2.0.0/24 -d 10.3.0.0/24  -j ACCEPT;
iptables -t nat -A POSTROUTING -o eth0 -s 10.3.0.0/24 -d 10.2.0.0/24  -j ACCEPT;
iptables -t nat -A PREROUTING  -i eth0  -j DNAT -p tcp -d 10.1.0.10/32 --dport 8081 --to-destination 10.2.0.10:80;
iptables -t nat -A POSTROUTING -o eth0  -j SNAT -p tcp -d 10.2.0.10/32 --dport 80  --to-source 10.1.0.10;
iptables -t nat -A PREROUTING  -i eth0 -j DNAT -p tcp -d 10.1.0.10/32 --dport 8082 --to-destination 10.3.0.10:80;
iptables -t nat -A POSTROUTING -o eth0 -j SNAT -p tcp -d 10.3.0.10/32 --dport 80  --to-source 10.1.0.10;
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE;
```

Let's discuss briefly the logic of iptables configuration.<br>
To allow traffic to localhost, type this command:
```console
iptables -A INPUT -i lo -j ACCEPT
```
we use **lo** or loopback interface. The command makes sure that the connections between applications inside the same machine are working properly.

To accept SSH incoming connection in nva:
```console
iptables -A INPUT -p tcp --dport ssh -j ACCEPT
```

A rule that uses connection tracking to accept in INPUT only the packets that are associated with an established connections:
```console
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT
```
To accept incoming ping:
```console
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT
```

The DROP target on INPUT chain should be added after defining –dport rules. This will prevent an unauthorized connection from accessing to the target VM via other open ports:
```console
iptables -A INPUT -j DROP
```

Regardless of their destination, when packets match a particular rule in one of the tables, a _target_ or action is applied to them. <br> 
If the rule specifies an **ACCEPT** _target_ for a matching packet, the packet skips the rest of the rule checks and is allowed to continue to its destination. <br>
If a rule specifies a **DROP** _target_, that packet is refused access to the system and nothing is sent back to the host that sent the packet. <br>
If a rule specifies the optional **REJECT** _target_, the packet is dropped, but an error packet is sent to the packet's originator.

Let's discuss the NAT rules.<br>
In the nat table the top rules of PREROUTING and POSTROUTING chains are:
 ```console
iptables -t nat -A PREROUTING -i eth0 -s 10.2.0.0/24 -d 10.3.0.0/24  -j ACCEPT;
iptables -t nat -A PREROUTING -i eth0 -s 10.3.0.0/24 -d 10.2.0.0/24  -j ACCEPT;
iptables -t nat -A POSTROUTING -o eth0 -s 10.2.0.0/24 -d 10.3.0.0/24  -j ACCEPT;
iptables -t nat -A POSTROUTING -o eth0 -s 10.3.0.0/24 -d 10.2.0.0/24  -j ACCEPT;
 ```
 Those rules avoid address traslation between the vnet2 and vnet3. When the rule is matched with **-j ACCEPT** , the packet is accepted and the evaluation of the reamin rules in the chain are stopped.


**Destination NAT** is done in the <ins>PREROUTING</ins> chain, just as the packet comes in.<br>
**-i** option identify the incoming interface<br>
Destination NAT is specified using `-j DNAT`, and the `--to-destination` option specifies an IP address, a range of IP addresses, and an optional port or range of ports (for UDP and TCP protocols only).
The <ins>PREROUTING</ins> chain in NAT specifies a destination IP address and port where incoming packets requesting a connection to your internal service can be forwarded. For example, if you wanted to forward incoming HTTP requests from internet with destination port 8081 to your internal nginx HTTP Server at 10.2.0.10 on destination port 80, run the following command:<br>
**iptables -t nat -A PREROUTING  -i eth0  -j DNAT -p tcp -d 10.1.0.10/32 --dport 8081 --to-destination 10.2.0.10:80**

**Source NAT** changes the source address of connections to something different. This is done in the <ins>POSTROUTING</ins> chain, just before it is finally sent out. This is an important detail, since it means that anything else on the Linux box itself (routing, packet filtering) will see the packet unchanged. It also means that the `-o` (outgoing interface) option can be used.
Source NAT is specified using `-j SNAT`, and the `--to-source` option specifies an IP address, a range of IP addresses, and an optional port or range of ports (for UDP and TCP protocols only). <ins>POSTROUTING</ins> chain allows packets to be altered as they are leaving the nva outgoing eth0 interface. <br>
**iptables -t nat -A POSTROUTING -o eth0  -j SNAT -p tcp -d 10.2.0.10/32 --dport 80  --to-source 10.1.0.10**

Below the network diagram with visualization of inbound traffic from internet to **vmApp2**:
[![6]][6]

Below the network diagram with visualization of inbound traffic from internet to **vmApp3**:
[![7]][7]

The diagram show the traffic between **vmApp2** and **vmApp3** with transit through the NVAs:

[![8]][8]

IP Masquerade, called "IPMASQ" or "MASQ" for short, is a form of Network Address Translation (NAT) which allows internally connected hosts that do not have one or more registered Internet IP addresses to communicate to the Internet.
In the case of IPMASQ, a gateway machine acts as the mediator between the machines on your network and the Internet. Connection Tracking (contract) feature of Linux is used to keep track of connections and their source. This helps in rerouting the packets accordingly. Henceforth, packets leaving the private network are masqueraded as if they originated from the mediator machine (NVA). Masquerading takes care to re-map IP addresses and ports as required.
The traffic from the **vmApp2** and **vmApp3** can access to internet by:
```
iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE;
```
The rule uses the NAT packet matching table (-t nat) and specifies the built-in POSTROUTING chain for NAT (-A POSTROUTING) on the nva external networking device (-o eth0). POSTROUTING allows packets to be altered as they are leaving the firewall's external device. The -j MASQUERADE target is specified to mask the private IP address of a node with the external IP address of the nva.



### iptables configuration persistent to the VM reboot
In order to make your iptables rules persistent after reboot, install the **iptables-persistent** package using the apt package manager:
```bash
apt install iptables-persistent
```
The current iptables rules can be saved to the corresponding IPv4 and IPv6 files below:
```bash
/etc/iptables/rules.v4
/etc/iptables/rules.v6
```
In our setup we use only IPv4 rules.

After a rule change, to make changes permanent after reboot, run **iptables-save** command:
```bash
sudo /sbin/iptables-save > /etc/iptables/rules.v4
OR
sudo /sbin/ip6tables-save > /etc/iptables/rules.v6
```

### NOTE
Installing **iptables-persistent** on ubuntu without manual input requires the setting of flags in **debconf-set-selections**
```bash
echo iptables-persistent iptables-persistent/autosave_v4 boolean true | sudo debconf-set-selections
echo iptables-persistent iptables-persistent/autosave_v6 boolean true | sudo debconf-set-selections
sudo apt-get -y install iptables-persistent
```

You can verify these fields by installing debconf-utils and searching for iptables values:
```
sudo apt install debconf-utils
sudo debconf-get-selections | grep iptables
```

## <a name="List of files"></a>3. List of files 
| file                  | Description                                                  | 
| --------------------- |------------------------------------------------------------- | 
| **init.json**         | input parameter file defining: Azure subscription, Resource group name, Azure region, administrator credential|
| **az.json**           | ARM template to deploy load balancers and VMs                |
| **az.ps1**            | powershel script to deploy **az.ps1**                        |


**NOTE:** 
- **Before running the ARM template, customize the values of input variables in init.json file**
- **To deploy the full solution, run "az.json"**
- installation of nginx in all VMs, configuration of ip fowarding and iptables in NVA is executed through custom script extension

<br>

Meaning of variables in **init.json**:
```json
{
    "subscriptionName": "NAME_OF_AZURE_SUBSCRIPTION",
    "ResourceGroupName": "NAME_OF_RESOURCE_GROUP",
    "location": "AZURE_LOCATION_NAME",
    "adminUsername": "ADMINISTRATOR_USERNAME",
    "authenticationType": "password",
    "adminPasswordOrKey": "ADMINISTRATOR_PASSWORD"
}
```

## <a name="check"></a>4. Check of flows through NVAs
Using tcpdump is easy to verify the NAT and symmetrical transit through the NVAs:

```bash
root@nva1:~# tcpdump -nqt host 10.2.0.10 or host 10.3.0.10
root@nva2:~# tcpdump -nqt host 10.2.0.10 or host 10.3.0.10
```
By curl the vmApp1 can query an interfnet web site or the **vmApp3**:
```bash
root@vmApp1:~# curl http://www.microsoft.com
root@vmApp1:~# curl 10.3.0.10
```


## <a name="iptables"></a>5. ANNEX: iptables commands 

### How to show the current rules
```console
iptables -nvL
```
**-L** option is used to list all the rules <br>
**-v** option is for showing the info in a more detailed format <br>

List rules inclusive of line number:
```console
iptables -n -L -v --line-numbers
```
```console
iptables -t nat -L
iptables -t nat -L -nv
iptables --list --line-numbers
```

### How to flush and reset the iptables to the default  
```console
iptables -F
iptables -X
iptables -t nat -F
iptables -t nat -X
iptables -t mangle -F
iptables -t mangle -X
iptables -t raw -F
iptables -t raw -X
iptables -t security -F
iptables -t security -X
iptables -P INPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -P OUTPUT ACCEPT
```
**-F** command with no arguments flushes all the chains in its current table. <br>
**-X** deletes all empty non-default chains in a table

**iptables -t nat -F**       &nbsp;&nbsp;&nbsp;flush the nat table <br>
**iptables -t mangle -F**    &nbsp;&nbsp;&nbsp;flush the mangle table <br>
**iptables -F**              &nbsp;&nbsp;&nbsp;flush all chains <br>
**iptables -X**              &nbsp;&nbsp;&nbsp;delete all non-default chains <br>
**iptables -Z**              &nbsp;&nbsp;&nbsp;clear the counters for all chains and rules <br>


`Tags: iptables` <br>
`date: 30-10-22`

<!--Image References-->

[1]: ./media/network-diagram1.png "network diagram"
[2]: ./media/network-diagram2.png "network diagram with inbound traffic from internet to the vmApp2"
[3]: ./media/network-diagram3.png "network diagram with inbound traffic from internet to the vmApp3"
[4]: ./media/network-diagram4.png "network diagram with internal traffic between vnets"
[5]: ./media/iptables.png "iptables: flow of packets through the chains"
[6]: ./media/nat1.png "NAT applied to the traffic inbound from internet to vmApp2"
[7]: ./media/nat2.png "NAT applied to the traffic inbound from internet to vmApp3"
[8]: ./media/traffic-between-vnets.png "no NAT is applied for traffic between vnet2 and vnet3 in transit through the NVAs"

<!--Link References-->

